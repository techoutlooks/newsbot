from daily_query.helpers import mk_date
from newsutils.base.posts import PostConfigMixin
from newsutils.nlp import DayNlp
from newsutils import META_POST

from scrapy.commands import ScrapyCommand
from scrapy.exceptions import UsageError
from scrapy.utils.conf import arglist_to_dict

from daily_query.mongo import MongoDaily, Collection
from newsutils.logging import log_running, NamespaceFormatter

# type of summary to generate/save, whether one per post, or
# a meta summary (summary of summaries) across all sibling posts.
CMD_VERBS = ('similarity', 'summary', META_POST)


class NlpCmd(PostConfigMixin, ScrapyCommand):
    """
    Compute similarity scores from daily articles and save them
    to the db under the `POSTS['SIBLINGS_FIELD'] and POSTS['RELATED_FIELD'] settings
    Usage:
        >>> scrapy nlp                  # today's articles
        >>> scrapy nlp -d 2021-08-28    # specific day, default thresholds
        >>> scrapy nlp -t siblings=0.4 -t related=0.2 -d 2021-08-28 -d 2021-08-29
        >>> nlp metapost -t siblings=0.10 -t related=0.05 -D from=2022-05-20


    # TODO: try more sophisticated methods eg. LDA,
        instead of relying on newspaper3k/goose3 extractive summary (`summary` field).
    """

    # whether to use the post extractive summary generated by newspaper3k/goose3
    # instead of the post text, in all NLP tasks (summarization, categorization)
    # `summary_as_text = False` yield better summaries in practice.
    summary_as_text = False
    meta_summary_uses_nlp = False

    # db handle
    daily = MongoDaily(PostConfigMixin.settings["CRAWL_DB_URI"])

    # ScrapyCommand overrides
    requires_project = True
    log_prefix = "nlp"  # picked up by the logger. cf `LoggingMixin`.

    def short_desc(self):
        return "Update day's (default, today) articles with similarity scores"

    def syntax(self):
        return f'<{"|".join(filter(None, CMD_VERBS))}> [options]'

    def add_options(self, parser):

        ScrapyCommand.add_options(self, parser)

        parser.add_option(
            "-D", "--days", dest="days_range", action="append", default=[], metavar="NAME=VALUE",
            help="articles matching date range; eg. -D from=2022-03-19 [to=2022-04-22]."
                 "no option supplied defaults to today's articles.")
        parser.add_option(
            "-d", "--day", dest="days_list", action="append", default=[], metavar="DAY",
            help=f"articles for given day only; eg. `-d 2021-06-23. "
                 f"(default: -d {mk_date()})"),
        parser.add_option(
            "-t", "--threshold", dest="thresholds", action="append", default=[], metavar="NAME=VALUE",
            help=f"articles matching given dates only. (default: "
                 f" -t siblings={self.settings['POSTS']['SIMILARITY_SIBLINGS_THRESHOLD']})"
                 f" -t related={self.settings['POSTS']['SIMILARITY_RELATED_THRESHOLD']})")
        # TODO: add option parsing for `SIMILARITY_MAX_DOCS`
        #   cmd options override in `process_options`

    def process_options(self, args, opts):

        ScrapyCommand.process_options(self, args, opts)
        try:
            opts.days_range = arglist_to_dict(opts.days_range)
            opts.thresholds = arglist_to_dict(opts.thresholds)
        except ValueError as e:
            print(e)
            raise UsageError("Invalid -a value, use -a NAME=VALUE", print_help=False)

        # override project settings
        # iff supplied by cmdline
        for k, v in dict(
                SIMILARITY_SIBLINGS_THRESHOLD=opts.thresholds.get(self.siblings_field),
                SIMILARITY_RELATED_THRESHOLD=opts.thresholds.get(self.related_field),
        ).items():
            if v:
                self.settings.set(k, float(v), priority='cmdline')

    def run(self, args, opts):

        # perform all actions by default
        verb = args[0] if args else None
        assert verb in (*CMD_VERBS, None), \
            f"unsupported verb: {verb}. " \
            f"valid actions: {', '.join(CMD_VERBS)}"

        self.save(
            verb=verb,
            days={'days_from': opts.days_range.get('from'),
                  'days_to': opts.days_range.get('to'),
                  'days': opts.days_list}
        )

    def save(self, verb, days):
        """
        Computes and store articles similarity scores for given dates.
        :params day: str|date: the articles' date, eg. `2021-06-22`
        """

        stats = dict(
            total=0, saved=0, words=0,
            similarity=0, summary=0, metapost=0)

        fmt = NamespaceFormatter({"from": None, "to": None})
        msg = fmt.format("`scrapy nlp` (docs from {days_from} to {days_to}) +{days}", **days)

        # ensure day correspond to an existing database collection
        collections, stats['total'] = self.daily.get_collections(**days)
        days_names = [str(d) for d in collections]
        for day in collections:

            # run per day
            # day_counts = self.save_day(day, verb)
            save_day = lambda cmd, *args, **kwargs: \
                cmd.save_day(*args, **kwargs)
            day_counts = log_running(f"nlp {verb or 'all'} {day}", msg)\
                (save_day)(self, day, verb)

            for s in list(stats):
                stats[s] += day_counts[s]

        self.log_task_ended(
            "saved ({saved}) fields in ({total}) docs, ({words}) words: "
            "generated (similarity/summary/metapost): {similarity}/{summary}/{metapost} posts. "
            f"published on: {days_names}.", **stats)

    def save_day(self, collection: Collection, verb: str):
        """ Perform NLP (similarity, title/text summary, metapost generation)
        and save the results to the database """

        nlp = DayNlp(collection)
        return nlp.save_day(verb)





