# leeram-newsbot

News crawling and semantic analysis using NLP. 
Generate original title/content/category from any source using NLP techniques.

Provides following two components for news fetching:
* `newsbot.crawler` leverages Scrapy to crawl news and run NLP-related commands.
* `newsbot.ezines` downloads news by performing http requests to supported endpoints.

News publishing:
* `newsbot`


## Features (check the demo )
- Create a spider with few lines of code
- Multiple crawling rules per site, mapping to different post categories.
  Cf. `post_categories_xpaths`
- Detect post content revision => `.version` field of post incremented.
- Bulk crawl news sites with a single command `crawlall`
- Compute posts similarity across posts by all crawled sites (published on same date). 
  `nlp` command.

## TODO

- publishing stats (`crawler.publish.stats`)
- similarity across docs belonging to several days 


## Dev


### Requirements

* deps: `pip-tools==6.6.2`, `click=7.1.2` 
* started MongoDB instance
    ```shell
    docker run --name mongo --restart=unless-stopped -d -p 27017:27017  -v mongodata:/data mongo 
    ```

### Setup

* Install pip-tools
```shell
pip install -U pip-tools
```

* (Re-)create dev requirements files. 
**Important!**: Re-run required before (re-)building Docker the image
```shell
# run from the `src` folder.
pip-compile --resolver=backtracking requirements/in/prod.txt --output-file requirements/prod.txt 
pip-compile --resolver=backtracking requirements/in/dev.txt  --output-file requirements/dev.txt 
```
* Sync dev requirements to venv
```shell
# run from `src` folder.
pip-sync src/requirements/dev.txt 
# pip-sync requirements/prod.txt requirements/dev.txt
```
* Env vars setup

```shell

# newsboard frontend url=http://localhost:3100
export \
  METAPOST_BASEURL='/posts' \
  PROJECT_SETTINGS_MODULE=crawler.settings
```


### Quick start

* Run all tasks at once, periodically. \
This runs all spiders, fetches sport news, and run NLP tasks.
    ```shell
    python run.py
    ```
  
* Run Scrapy commands individually, eg.:
    ```shell
    cd crawler 
    python scrapy crawlall 
    scrapy nlp
    
    # every 5mn, publish given day's posts to all channels  
    CRAWL_SCHEDULE=5 scrapy publish -d 2023-05-19

    ```

### Syntax

Note: -D: for date ranges, -d: for single dates


1. run all spiders
    ```  
     scrapy crawlall \
      [-D from=<%Y-%m-%d> -D to=<%Y-%m-%d>] \ 
      [-d <%Y-%m-%d>] \
    ```

2. update similarity based on thresholds (-t option)
    ```  
     scrapy nlp \
      [-D from=<%Y-%m-%d> -D to=<%Y-%m-%d>] \ 
      [-d <%Y-%m-%d>] \
      [-t siblings=<%f>] [-t related=<%f>]
    ```

### Examples

1. set env and cwd properly
    ```shell
    cd newsbot/src/
    source venv/bin/activate
   
    export 
        PROJECT_SETTINGS_MODULE=crawler.settings \
        POSTS=metapost_baseurl=http://localhost:3100/posts \
        PUBLISH=facebook_page_id\=114619074914814,facebook_page_access_token\=EAAwyeRawKuUBANX0rMywMrLHHgZAQbT90pddXLp8jZBaZBfM6YP0AWGWHnk4SppELnjTt3vCJtZBcJbZCJkWpoWSwez77uQaZAkJx6WUrO7MQJUmfToHPro1h691V1E55AAOUEXhSK6xrPYMaSWEhC8tQ6qZAHDhMfJNgtrQJhQTrfZBMaa2fRu6
    
    python crawler/commands/nlp.py -t siblings=0.40 -t related=0.2 -d 2022-03-20

    ```

2. crawl all spiders in the `crawler/spiders` folder
    ```shell
    
    # crawl all posts with regardless their publish time  
    scrapy crawlall
    
    # crall posts matching specified date range (note the -D option)
    scrapy crawlall --loglevel INFO -D from=2022-03-09 -D to=2022-03-09 
    
    # from forever to 2022-04-21
    scrapy crawlall -D to=2022-04-21
    
    # from 2022-04-19 to today
    scrapy crawlall -D from=2022-04-19
    
    # mixed date range and days
    scrapy crawlall -d 2022-04-09 -d 2022-04-10 -d 2022-04-24 -D from=2022-04-19 -D to=2022-04-21          
    ```

3. Run NLP tasks

Cf. `TextSummarizer`, `TitleSummarizer`, `Categorizer` models from our
[newsnlp](https://github.com/techoutlooks/newsnlp.git) package (dependency).

`scrapy nlp [similar|summary|metapost] ...` for computing and saving per each post in the db:
- a similarity score vs. other posts published the same day;
  similarity meant for `siblings` or `related` posts of any given post.
- a summary from the post's text and title
- a meta post from the post's siblings, generated by compiling then summarizing the titles and
  texts of all posts.


```shell
# update posts similarity
scrapy nlp similarity -t siblings=0.10 -t related=0.05 -d 2022-05-16

# generate and save a summary for each post
scrapy nlp summary -t siblings=0.10 -t related=0.05 -d 2022-05-16

# generate and save meta posts
scrapy nlp metapost -t siblings=0.10 -t related=0.05 -d 2022-05-16
```


Playing around with dates

```shell
# single dates
scrapy nlp -t siblings=0.40 -t related=0.2 -d 2022-03-07 -d 2022-03-19  

# date range 
scrapy nlp -t siblings=0.40 -t related=0.2 -D from=2022-03-19            

# mixed date range and days
scrapy nlp -t siblings=0.35 -t related=0.15 -D from=2022-03-19 -d 2022-03-02

```

### Env vars

Following env vars with respective defaults supported by the project: 

* `newsbot.crawler`
  - CRAWL_DB_URI=mongodb://db:27017/scraped_news_db
  - SIMILARITY_SIBLINGS_THRESHOLD=0.4
  - SIMILARITY_RELATED_THRESHOLD=0.2


* `newsbot.ezines` 
  - TIMEOUT=1 - api requests timeout (seconds)


* `src/run.py` script
  - CRAWL_DAYS_FROM - same as `crawlall -D from=<from-date>`
  - CRAWL_DAYS_TO - same as `crawlall -D to=<to-date>`
  - CRAWL_DAYS - same as `crawlall -d <date>`
  - CRAWL_SCHEDULE=20 - interval (minutes) between crawl+nlp jobs

### Docker

Below still to check

```shell
CRAWL_SCHEDULE=20 # runs every 20mn
docker-compose run --service-ports newsbot \
  "cd crawler/ && scrapy crawlall && scrapy nlpsimilarity -t siblings=0.4 -t related=0.2 -d 2021-10-22 -d 2021-10-23"
```


## Debugging

* [debugging hints](./doc/debug.md).


## Prod

* Getting [ready for GCP](./doc/gcloud-init.md). Optional, do once per project) 
* Run project as a [gcloud run job](./doc/gcloud.md).


## TODO

* [Fixes](./doc/todo.md#fixme).
* [Future](./doc/todo.md#todo).
* Run dev and Docker image in conda rather than venv. cf `newsnlp` dep
